[[Article Note]]

[PDF](files/fileDoesNotExist/notExistingFile)

## Why 
*Why did I choose to read this article to read?*

Recommended by Dr. Liu
Techniques may inspire the design of robust GNN  

## Background
*Relavent researches / the problem*

In the presence of Byzantine machines during a gradient descent, the "time-series" gradients of *one* machine can be examined. By comparing the
- **gradinet** and more novelly 
- **Averaged Difference** between stochastic gradient *calculated by a single machine* and the actually adopted gradient in the past iterations (which, for good machines, should converge as $1\over \sqrt{N}$ due to central limit theorem)

## Method



## Contribution



## Summary


## My Thoughts